import io
import contextlib
import pandas as pd
import streamlit as st
import matplotlib.pyplot as plt
import google.generativeai as genai

# Funci√≥n para iniciar el chat
def iniciar_chat(df):
    model = genai.GenerativeModel('gemini-2.0-flash')
    chat = model.start_chat(history=[
        {
            "role": "user",
            "parts": ["Tienes un DataFrame de pandas llamado df. Estas son las columnas reales que contiene: " + ", ".join(df.columns) + ". No traduzcas ni cambies ning√∫n nombre de columna. Usa los nombres tal como est√°n."]
        },
        {
            "role": "model",
            "parts": ["Entendido. Usar√© los nombres de columna exactamente como los proporcionaste."]
        }
    ])
    st.session_state.chat = chat
    # Inicializar el historial si no existe
    if 'history' not in st.session_state:
        st.session_state.history = [
            {"role": "system", "content": "üü¢ Asistente activo. Pregunta lo que quieras sobre tu DataFrame."},
            {"role": "system", "content": "‚úèÔ∏è Escribe 'salir' para finalizar."}
        ]

# Funci√≥n para mostrar el historial de conversaci√≥n
def mostrar_historial():
    for msg in st.session_state.history:
        if msg["role"] == "user":
            st.markdown(f"**Usuario**: {msg['content']}")
        elif msg["role"] == "assistant":
            st.markdown(f"**Asistente**: {msg['content']}")
            if "figure" in msg:
                st.pyplot(msg["figure"])
            elif "result_df" in msg:
                st.dataframe(msg["result_df"])
        else:
            st.markdown(f"{msg['content']}")

# Funci√≥n para procesar la pregunta y generar la respuesta
def procesar_pregunta(pregunta, df):
    if pregunta.lower() == "salir":
        st.session_state.history.append({"role": "system", "content": "üõë Chat finalizado."})
        return

    # Guardar la pregunta en el historial
    st.session_state.history.append({"role": "user", "content": pregunta})

    prompt = f"""
Tienes un DataFrame de pandas llamado df cargado en memoria.
Estas son las columnas reales: {', '.join(df.columns)}.
NO CAMBIES los nombres de las columnas.

Responde a esta pregunta escribiendo solamente el c√≥digo Python que RETORNA la respuesta. NO uses print() ni muestres la salida directamente; solo retorna el resultado.

Para preguntas sobre productos, como 'urea', usa b√∫squedas flexibles que ignoren may√∫sculas/min√∫sculas (por ejemplo, .str.contains('urea', case=False, na=False)) y consideren variaciones del texto (por ejemplo, 'Urea 46%', 'urea granulada').

Si la pregunta requiere una gr√°fica, genera la gr√°fica usando matplotlib y mu√©strala con plt.figure(). En este caso, retorna None.

Si la pregunta pide mostrar el DataFrame o una tabla (por ejemplo, 'muestra las primeras 5 filas'), retorna el DataFrame directamente (por ejemplo, df.head(5)).

Pregunta:
{pregunta}
"""
    try:
        response = st.session_state.chat.send_message(prompt)
        code = response.text.strip("```python").strip("```").strip()

        if not code:
            st.session_state.history.append({"role": "assistant", "content": "‚ùå **No se gener√≥ c√≥digo**. Intenta preguntar de otra forma."})
            return

        buffer = io.StringIO()
        exec_globals = {"df": df, "plt": plt, "pd": pd}
        fig = None

        with contextlib.redirect_stdout(buffer):
            try:
                # Ejecutar el c√≥digo para capturar gr√°ficas
                exec(code, exec_globals)
                if plt.get_fignums():
                    fig = plt.gcf()
                plt.close('all')
            except Exception as e:
                st.session_state.history.append({"role": "assistant", "content": f"‚ùå **Error al ejecutar el c√≥digo**: {str(e)}"})
                return

        # Armar la respuesta sin mostrar el c√≥digo
        DEBUG_MODE = False
        response_dict = {"role": "assistant", "content": ""}
        if DEBUG_MODE:
            response_dict["content"] += f"üíª **C√≥digo ejecutado**:\n```python\n{code}\n```"

        if fig:
            response_dict["figure"] = fig
            response_dict["content"] += "üìä **Gr√°fica generada:**"
        else:
            try:
                # Evaluar el c√≥digo para obtener el resultado
                result = eval(code, {"df": df, "pd": pd})
                # Convertir el resultado en DataFrame si no lo es
                if isinstance(result, pd.DataFrame):
                    result_df = result
                elif isinstance(result, (list, tuple)):
                    result_df = pd.DataFrame(result, columns=["Resultado"])
                elif isinstance(result, (int, float, str)):
                    result_df = pd.DataFrame({"Resultado": [result]})
                elif result is None:
                    # Si el resultado es None, intentar manejar casos como df.head()
                    if "head(" in code or "tail(" in code or "df[" in code or "df." in code:
                        # Re-ejecutar el c√≥digo en un entorno controlado para capturar el DataFrame
                        result_df = eval(code, {"df": df, "pd": pd})
                        if not isinstance(result_df, pd.DataFrame):
                            result_df = pd.DataFrame({"Resultado": ["No se pudo obtener un DataFrame"]})
                    else:
                        result_df = pd.DataFrame({"Resultado": ["No se retorn√≥ ning√∫n valor"]})
                else:
                    result_df = pd.DataFrame({"Resultado": [str(result)]})

                # Redondear n√∫meros a 2 decimales para columnas num√©ricas
                for col in result_df.select_dtypes(include=['float64', 'float32']).columns:
                    result_df[col] = result_df[col].round(2)

                response_dict["result_df"] = result_df
                response_dict["content"] += "\nüìã **Resultados:**"
            except Exception as e:
                # Si no se puede evaluar, usar la salida de texto como DataFrame
                output = buffer.getvalue().strip()
                if output:
                    result_df = pd.DataFrame({"Resultado": [output]})
                else:
                    result_df = pd.DataFrame({"Resultado": ["No se obtuvo ninguna salida"]})
                response_dict["result_df"] = result_df
                response_dict["content"] += "\nüìã **Resultados:**"

        # Guardar la respuesta en el historial
        st.session_state.history.append(response_dict)

    except Exception as e:
        st.session_state.history.append({"role": "assistant", "content": f"‚ùå **Algo sali√≥ mal con la consulta. Detalles**: {str(e)}"})

# Funci√≥n para borrar el historial del chat
def borrar_historial():
    if st.button('Borrar chat'):
        st.session_state.history = [
            {"role": "system", "content": "üü¢ Chat borrado. Comienza una nueva conversaci√≥n."},
            {"role": "system", "content": "‚úèÔ∏è Escribe 'salir' para finalizar."}
        ]
        st.experimental_rerun()  # Refrescar la p√°gina para reflejar el historial limpio
