import io
import contextlib
import pandas as pd
import streamlit as st
import matplotlib.pyplot as plt
import google.generativeai as genai
import plotly.express as px
import plotly.graph_objects as go

# Funci√≥n para iniciar el chat
def iniciar_chat(df):
    model = genai.GenerativeModel('gemini-2.0-flash')
    chat = model.start_chat(history=[
        {
            "role": "user",
            "parts": ["Tienes un DataFrame de pandas llamado df. Estas son las columnas reales que contiene: " + ", ".join(df.columns) + ". No traduzcas ni cambies ning√∫n nombre de columna. Usa los nombres tal como est√°n."]
        },
        {
            "role": "model",
            "parts": ["Entendido. Usar√© los nombres de columna exactamente como los proporcionaste."]
        }
    ])
    st.session_state.chat = chat
    st.session_state.history = [
        "üü¢ Asistente activo. Pregunta lo que quieras sobre tu DataFrame.",
        "‚úèÔ∏è Escribe 'salir' para finalizar."
    ]

# Funci√≥n para mostrar el historial de conversaci√≥n
def mostrar_historial():
    for msg in st.session_state.history:
        st.write(msg)

# Funci√≥n para procesar la pregunta y generar la respuesta
def procesar_pregunta(pregunta, df):
    prompt = f"""
Tienes un DataFrame de pandas llamado df cargado en memoria.
Estas son las columnas reales: {', '.join(df.columns)}.
NO CAMBIES los nombres de las columnas.

Responde a esta pregunta escribiendo solamente el c√≥digo Python que da la respuesta.

Para preguntas sobre productos, como 'urea', usa b√∫squedas flexibles que ignoren may√∫sculas/min√∫sculas (por ejemplo, .str.contains('urea', case=False, na=False)) y consideren variaciones del texto (por ejemplo, 'Urea 46%', 'urea granulada').

Si la pregunta requiere una gr√°fica, genera la gr√°fica usando matplotlib y mu√©strala con st.pyplot().

Pregunta:
{pregunta}
"""
    try:
        # Aqu√≠ estamos capturando el texto de la respuesta del modelo
        response = st.session_state.chat.send_message(prompt)
        code = response.text.strip("```python").strip("```").strip()
        
        if not code:
            st.session_state.history.append("‚ùå **No se gener√≥ c√≥digo**. Intenta preguntar de otra forma.")

        buffer = io.StringIO()
        exec_globals = {"df": df, "plt": plt}  # Aseg√∫rate de incluir plt en exec_globals
        
        with contextlib.redirect_stdout(buffer):
            try:
                exec(code, exec_globals)
            except Exception as e:
                st.session_state.history.append(f"‚ùå **Error al ejecutar el c√≥digo**: {str(e)}")
                return  # Salir de la funci√≥n si hay error al ejecutar el c√≥digo
        
        output = buffer.getvalue()

        if output.strip():
            if "plt.show()" in code:
                st.session_state.history.append("üìä **Gr√°fica generada:**")
                st.pyplot()  # Aseg√∫rate de mostrar la gr√°fica con st.pyplot()
            else:
                result_df = pd.DataFrame([output.split("\n")]).T
                result_df.columns = ["Resultados"]
                st.session_state.history.append("üí¨ **Respuesta:**")
                st.session_state.history.append(result_df)

    except Exception as e:
        # Si algo falla en el proceso de la conversaci√≥n, mostramos el error
        st.session_state.history.append(f"‚ùå **Algo sali√≥ mal con la consulta. Detalles**: {str(e)}")

# Funci√≥n para borrar el historial del chat
def borrar_historial():
    if st.button('Borrar chat'):
        st.session_state.history = ["Chat borrado. Comienza una nueva conversaci√≥n."]
